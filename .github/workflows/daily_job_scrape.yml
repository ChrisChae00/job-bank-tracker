name: Daily Job Scrape

on:
  schedule:
    - cron: "10 15 * * *" # Runs daily at 10:10am EST (min, hour, day, month, day of week)
  workflow_dispatch: # Allows manual trigger
  # Triggers on pull requests to main branch
  pull_request:
    branches:
      - main

jobs:
  scrape_jobs:
    runs-on: ubuntu-latest # Runs on latest Ubuntu
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref || 'main' }}
          fetch-depth: 0 # Fetch full history for proper git operations

      # Builds the Docker image
      - name: Build Docker image
        run: docker build -t job-scraper .

      # Run scraper with existing DB mounted into the container
      # This allows incremental scraping and cleaning
      - name: Run scraper and cleaner in Docker
        run: |
          mkdir -p data
          docker run --name scraper-container \
            -v ${{ github.workspace }}/data:/app/data \
            job-scraper

      # Copy the updated database from container (ensure permissions/ownership are correct)
      - name: Copy results from Docker
        run: |
          docker cp scraper-container:/app/data/job_listings.db ./data/job_listings.db || echo "No job_listings.db found"

      # Commits and pushes the changes to the repository
      # Skip on pull_request events since we can't push to the triggering branch
      - name: Commit and push changes
        if: github.event_name != 'pull_request'
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/job_listings.db
          git diff --staged --quiet || git commit -m "chore(db): daily job scrape and clean [skip ci]"
          git push origin main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
