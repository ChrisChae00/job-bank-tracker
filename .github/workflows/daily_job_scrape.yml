name: Daily Job Scrape

on:
  schedule:
    - cron: "10 15 * * *" # Runs daily at 10:10am EST (min, hour, day, month, day of week)
  workflow_dispatch: # Allows manual trigger
  # Triggers on pull requests to main branch
  pull_request:
    branches:
      - main

jobs:
  scrape_jobs:
    runs-on: ubuntu-latest # Runs on latest Ubuntu
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Builds the Docker image
      - name: Build Docker image
        run: docker build -t job-scraper .

      # Run scraper with existing DB mounted into the container
      # This allows incremental scraping and cleaning
      - name: Run scraper and cleaner in Docker
        run: |
          docker run --name scraper-container \
            -v ${{ github.workspace }}/job_listings.db:/app/job_listings.db \
            job-scraper

      # Copy the updated database from container (in case volume mount didn't persist)
      - name: Copy results from Docker
        run: |
          docker cp scraper-container:/app/job_listings.db ./job_listings.db || echo "No job_listings.db found"

      # Commits and pushes the changes to the repository
      - name: Commit and push changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add job_listings.db
          git commit -m "chore(db): daily job scrape and clean [skip ci]" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
